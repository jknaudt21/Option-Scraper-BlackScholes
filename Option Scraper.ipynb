{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option Data Scraper for Black Scholes\n",
    "This program will scrape the following information about **all** current call contracts for batches of companies in the S&P500. The program will write into a csv called `\"SNP.csv\"`:\n",
    "* Option price\n",
    "* Option strike price\n",
    "* Option time to maturity (in years)\n",
    "* Underlying stock price\n",
    "* Underlying stock dividend yield\n",
    "* Underlying stock implied volatility (this is the best proxy for a stock's volatility)\n",
    "\n",
    "Almost all data is scraped from Yahoo Finance, with the exception of implied volatility, which is scraped from AlphaQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime, time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import sched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTickers():\n",
    "    \"\"\"Returns the tickers for all the S&P500 companies using the Wikipedia page\n",
    "    Outputs: \n",
    "        tickers - list of tickers for every company in the S&P500\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find(\"table\") # tickers are contained in a table\n",
    "    tickers = []\n",
    "    for row in table.find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                tickers.append(cols[0].text.strip())\n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStockVol(ticker):\n",
    "    \"\"\"Returns a stock's 30-day implied volatility from alphaqueries\n",
    "    Inputs:\n",
    "        ticker     - a string representing a stock's ticker\n",
    "    Outputs: \n",
    "        volatility - implied volatility for the stock \n",
    "    \"\"\"\n",
    "    url = \"https://www.alphaquery.com/stock/\"+ ticker+ \"/volatility-option-statistics/30-day/iv-mean\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find(\"table\")\n",
    "    rows = table.find_all('tr') \n",
    "    volatility = float(rows[5].find_all('td')[1].text.strip()) # Specific table entry in AlphaQuery containing data\n",
    "    return volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStockData(ticker):\n",
    "    \"\"\"Returns a stock's price, dividend yield, and implied volatility\n",
    "    Inputs:\n",
    "        ticker      - a string representing a stock's ticker\n",
    "    Outputs: \n",
    "        stock_price - stock's price\n",
    "        div_yield   - stock's dividend yield\n",
    "        volatility  - stock's implied volatility\n",
    "    \"\"\"\n",
    "    url = \"https://finance.yahoo.com/quote/\"+ticker # Change url based on ticker\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    tables = soup.find_all(\"table\") # tables in Yahoo finance\n",
    "    \n",
    "    # Get stock price\n",
    "    rows = tables[0].find_all('tr') \n",
    "    stock_price = rows[3].find_all('td')[1].text.strip() # Item that returns ask price (4th row in a table)\n",
    "    # Extract ask price by parsing out other data\n",
    "    x = stock_price.find('x')\n",
    "    stock_price = float(stock_price[0:x].replace(\",\",\"\")) # delete any comma to cast to float\n",
    "    \n",
    "    # Get dividend yield\n",
    "    rows = tables[1].find_all('tr') \n",
    "    div_yield = rows[5].find_all('td')[1].text.strip()  # Item that returns ask yield (6th row in a table)\n",
    "    # Extract yield by parsing out other data\n",
    "    x = div_yield.find('(')\n",
    "    if \"N\" not in div_yield[x+1:-2]: # Only set dividend if not 'N/A'\n",
    "        div_yield = float(div_yield[x+1:-2])/100\n",
    "    else: \n",
    "        div_yield = 0\n",
    "        \n",
    "    # Get volatility\n",
    "    volatility = getStockVol(ticker)\n",
    "    \n",
    "    return stock_price, div_yield, volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDates(url):\n",
    "    \"\"\"Returns all valid option dates for a given Yahoo options url\n",
    "    Inputs:\n",
    "        url   - Yahoo finance options url\n",
    "    Outputs: \n",
    "        dates - list of dates (UNIX time) for the underlying ticker\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    dates = []\n",
    "    selector = soup.find(\"select\")\n",
    "    for item in list(soup.find(\"select\").children):\n",
    "        dates.append(int(item['value']))\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOptionData(url):\n",
    "    \"\"\"Returns a list of strike and call prices for a given Yahoo finance option url \n",
    "    Inputs:\n",
    "        url     - string representing Yahoo finance option url\n",
    "    Outputs: \n",
    "        strikes - list of all strike prices\n",
    "        prices  - list of all call prices\n",
    "    \"\"\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    table = soup.find(\"table\")\n",
    "    strikes = []\n",
    "    prices = []\n",
    "    for row in table.find_all('tr'): # Iterate through every table entry\n",
    "        cols = row.find_all('td')\n",
    "        if cols:\n",
    "            strikes.append(float(cols[2].text.strip().replace(\",\",\"\"))) # 3rd column is strike, kill comma to cast\n",
    "            # Use ask, bid, or last price depending on availability\n",
    "            ask  = cols[5].text.strip().replace(\",\",\"\")\n",
    "            bid  = cols[4].text.strip().replace(\",\",\"\")\n",
    "            last = cols[3].text.strip().replace(\",\",\"\")\n",
    "            \n",
    "            ask = float(ask) if ask != \"-\" else 0\n",
    "            bid = float(bid) if bid != \"-\" else 0\n",
    "            last = float(last) if last != \"-\" else 0\n",
    "            \n",
    "            if ask != 0:\n",
    "                prices.append(ask)\n",
    "            elif bid != 0:\n",
    "                prices.append(bid)\n",
    "            else:\n",
    "                prices.append(last)\n",
    "    return strikes, prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper\n",
    "Here we will iterate through every call contract available the given batch size in the S&P index. At the end we will append the entire dataset to an existing csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeData(startIndex, bs, rf, verbose = True):\n",
    "    \"\"\"Writes into a csv the option values for a batch of stock tickers. During execution may print url and company tickers\n",
    "    Inputs:\n",
    "        startIndex - company index to start scraping from (0 - 499)\n",
    "        bs         - integer representing the batch size to scrape\n",
    "        rf         - risk free rate\n",
    "        verbose    - boolean that determines if you want output to be printed\n",
    "    \"\"\"\n",
    "    if startIndex < 0 or startIndex >499:\n",
    "        raise Exception(\"Invalid start index!\")\n",
    "        \n",
    "    cols  = ['Stock Price', 'Strike Price', 'Maturity', 'Dividends', 'Volatility', 'Risk-free', 'Call Price']\n",
    "    results = pd.DataFrame(columns = cols)\n",
    "    RISK_FREE =  rf\n",
    "\n",
    "    tickers = getTickers() # list of company tickers\n",
    "    unixToday = int(time.time()) # Today's date used to calculate maturity \n",
    "    frames=[]  # list of dataframes - will be appended to a single df and exported to csv\n",
    "\n",
    "    # iterate through every ticker\n",
    "    for i,ticker in enumerate(tickers[startIndex:startIndex+bs]):\n",
    "        frame = pd.DataFrame(columns = cols) # fresh frame\n",
    "        \n",
    "        if verbose:\n",
    "            print(ticker, (i+startIndex))\n",
    "        \n",
    "        # Get stock data\n",
    "        stock_price, div_yield, volatility = getStockData(ticker)\n",
    "\n",
    "        # Start option extraction\n",
    "        url = \"https://finance.yahoo.com/quote/\"+ticker+\"/options\"\n",
    "        if verbose:\n",
    "            print(url)\n",
    "        dates = getDates(url)\n",
    "\n",
    "        # Firt entry receives special treament in case maturity is today\n",
    "        strikes, prices = getOptionData(url) \n",
    "        maturity = (dates[0] - unixToday)/(60*60*24*365.25) # Convert UNIX time difference to fraction of a year\n",
    "        if maturity <= 0:\n",
    "            maturity = 1e-5 # trivial maturity for options that expire today\n",
    "\n",
    "        # Insert data into a dataframe\n",
    "        frame['Strike Price'] = strikes\n",
    "        frame['Call Price'] = prices\n",
    "        frame['Stock Price'] = stock_price\n",
    "        frame['Dividends'] = div_yield\n",
    "        frame['Volatility'] = volatility\n",
    "        frame['Risk-free'] = RISK_FREE\n",
    "        frame['Maturity'] = maturity\n",
    "\n",
    "        frames.append(frame) # first entry to dataframes\n",
    "\n",
    "        # Loop through the rest of the option contracts\n",
    "        for date in dates[1:]:\n",
    "            frame = pd.DataFrame(columns = cols)\n",
    "            url = \"https://finance.yahoo.com/quote/\"+ticker+\"/options\"+\"?date=\"+str(date) # Special URL for future dates\n",
    "           \n",
    "            if verbose:\n",
    "                print(url)\n",
    "            \n",
    "            maturity = (date - unixToday)/(60*60*24*365.25) # Convert UNIX time difference to fraction of a year\n",
    "            strikes, call_prices = getOptionData(url)\n",
    "            # Add data to dataframe\n",
    "            frame['Strike Price'] = strikes\n",
    "            frame['Call Price'] =  call_prices\n",
    "            frame['Stock Price'] = stock_price\n",
    "            frame['Dividends'] = div_yield\n",
    "            frame['Volatility'] = volatility\n",
    "            frame['Risk-free'] = RISK_FREE\n",
    "            frame['Maturity'] = maturity\n",
    "            frames.append(frame)\n",
    "            \n",
    "        frames.append(results)\n",
    "        results = pd.concat(frames)\n",
    "        frames = []\n",
    "        if verbose:\n",
    "            print()\n",
    "            print('----------------------------------------------------')\n",
    "            print()\n",
    "    \n",
    "    # End of main loop, concatenate all frames and export to csv\n",
    "    results.to_csv('SNP.csv', mode = 'a', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop\n",
    "Iterate through the tickers. \n",
    "\n",
    "Change the value of `num_batches` to determine how many batches of tickers (of size `bs`) you'll scrape\n",
    "\n",
    "To avoid server denial for too many requests, we sleep for 500 seconds until next batch is scraped\n",
    "\n",
    "In the event of a server denial, the program will error out but some data will still have been written on the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cols  = ['Stock Price', 'Strike Price', 'Maturity', 'Dividends', 'Volatility', 'Risk-free', 'Call Price']\n",
    "results = pd.DataFrame(columns = cols)\n",
    "results.to_csv('SNP.csv', index = False)\n",
    "\n",
    "num_batches = 3\n",
    "bs = 10\n",
    "rf = 0.0088\n",
    "wait_period = 500\n",
    "verbose = True\n",
    "startIdx = 0\n",
    "\n",
    "for i in range(num_batches):\n",
    "    if (startIdx + (i*bs)) < (499 - bs): # only scrape data if we won't exceed the ticker list\n",
    "        scrapeData(startIdx+i*bs, bs, rf, verbose)\n",
    "        if verbose:\n",
    "            print(\"Waiting for to avoid server denial\")\n",
    "            print()\n",
    "        time.sleep(wait_period)\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
